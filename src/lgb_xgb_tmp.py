# -*- coding: utf-8 -*-
"""DEFAULT_PRED_V4_LGB_XGB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oHHcgKF9XlqIGM9lzDxxSvrrHl7ZOyBs

# Dependencies
"""

# !pip install bayesian-optimization

import pandas as pd
from lib_bc import *
import warnings
# warnings.filterwarnings('ignore')
import xgboost as xgb
import lightgbm as lgb
import warnings
from sklearn.model_selection import StratifiedKFold, KFold

# warnings.filterwarnings('ignore')

"""# Helpers

## Read Data Helper - `Data For Modeling (v221025)`
"""
from numpy import ndarray
from f_ndarray import mload, msave

train = mload('DATASET/train.npz')
test = mload('DATASET/test.npz')

x_train = train.drop(['isDefault'], axis=1)
y_train = train['isDefault']


x_test = test.drop(['isDefault'], axis=1)
y_test = test['isDefault']

# setting 5-folds cv
kf = KFold(n_splits=5, shuffle=True, random_state=42)

"""## Cutoff (proba -> clf)"""

"""# LightGBM"""
# Convert to lgb.Dataset
train_matrix_lgb = lgb.Dataset(x_train, label=y_train)
valid_matrix_lgb = lgb.Dataset(x_test, label=y_test)

"""## LGB(Tuned after Bayes tuning)"""
optimal_params = {
    'boosting_type': 'gbdt',
    'objective': 'binary',
    'metric': 'auc',
    'learning_rate': 0.01,
    'num_leaves': 18,
    'max_depth': 18,
    'min_data_in_leaf': 47,
    'min_child_weight': 6.4,
    'bagging_fraction': 0.98,
    'feature_fraction': 0.96,
    'bagging_freq': 67,
    'reg_lambda': 6,
    'reg_alpha': 6,
    'min_split_gain': 0.7,
    'nthread': 14,
    'seed': 42,
    'silent': True,
}
lgb_tuned = lgb.train(optimal_params,
                      train_set=train_matrix_lgb,
                      num_boost_round=20000,
                      verbose_eval=1000,
                      early_stopping_rounds=200)

"""## Performance"""

pred_lgb_optimal = lgb_tuned.predict(x_test, num_iteration=lgb_tuned.best_iteration)

"""#### ROC Curve"""

plot_roc(test_y_true=y_test, test_y_proba=pred_lgb_optimal, name='LightGBM(tuned)')

"""## LGB(tuned) - Fea Importance"""

features = x_train.columns.tolist()
pd.DataFrame({'Value': lgb_tuned.feature_importance(importance_type='gain'), 'Feature': features}).sort_values(
    by="Value", ascending=False).head(20)

features = x_train.columns.tolist()
pd.DataFrame({'Value': lgb_tuned.feature_importance(importance_type='split'), 'Feature': features}).sort_values(
    by="Value", ascending=False).head(20)

"""# XGBoost"""

# Convert to xgb.DMatrix
train_matrix_xgb = xgb.DMatrix(x_train, label=y_train)
valid_matrix_xgb = xgb.DMatrix(x_test, label=y_test)

xgb_guess_params = {'booster': 'gbtree',
                    'objective': 'binary:logistic',
                    'eval_metric': 'auc',
                    'gamma': 1,
                    'min_child_weight': 1.5,
                    'max_depth': 5,
                    'lambda': 10,
                    'subsample': 0.7,
                    'colsample_bytree': 0.7,
                    'colsample_bylevel': 0.7,
                    'eta': 0.04,
                    'tree_method': 'exact',
                    'seed': 42,
                    'nthread': 36,
                    "silent": True,
                    }

folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
oof = np.zeros(len(x_train))
xgb_pred = np.zeros(len(x_test))

for fold_, (trn_idx, val_idx) in enumerate(folds.split(x_train, y_train)):
    print("Fold {}".format(fold_ + 1))
    trn_data = xgb.DMatrix(x_train.iloc[trn_idx], y_train.iloc[trn_idx])
    val_data = xgb.DMatrix(x_train.iloc[val_idx], y_train.iloc[val_idx])

    clf = xgb.train(params=xgb_guess_params,
                    dtrain=trn_data,
                    num_boost_round=2000,
                    evals=[(trn_data, 'train'), (val_data, 'valid')],
                    maximize=False,
                    early_stopping_rounds=100,
                    verbose_eval=100)

    oof[val_idx] = clf.predict(xgb.DMatrix(x_train.iloc[val_idx]), ntree_limit=clf.best_ntree_limit)
    xgb_pred += clf.predict(xgb.DMatrix(x_test), ntree_limit=clf.best_ntree_limit) / folds.n_splits


fea_gain_dict = clf.get_score(importance_type='gain')
dict(sorted(fea_gain_dict.items(), key=lambda item: item[1], reverse=True))

from xgboost import plot_importance
from matplotlib import pyplot

'''
https://xgboost.readthedocs.io/en/latest/python/python_api.html
importance_type (str, default "weight") –
How the importance is calculated: either “weight”, “gain”, or “cover”
”weight” is the number of times a feature appears in a tree
”gain” is the average gain of splits which use the feature
”cover” is the average coverage of splits which use the feature where coverage is defined as the number of samples affected by the split
'''
plot_importance(clf, importance_type='gain')
pyplot.show()

fea_weight_dict = clf.get_score(importance_type='weight')
dict(sorted(fea_weight_dict.items(), key=lambda item: item[1], reverse=True))

fea_cover_dict = clf.get_score(importance_type='cover')
dict(sorted(fea_cover_dict.items(), key=lambda item: item[1], reverse=True))
